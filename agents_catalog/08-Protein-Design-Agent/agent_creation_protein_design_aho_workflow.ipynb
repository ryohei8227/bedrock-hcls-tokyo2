{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Protein Design Agent with AWS HealthOmics Workflow Integration\n",
    "\n",
    "This notebook demonstrates how to create a Bedrock agent that can trigger AWS HealthOmics workflows for protein design optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "1. Go through the notebook environment setup in the agents_catalog/0-Notebook-environment/ folder\n",
    "\n",
    "2. Deploy protein_design_stack.yaml to your AWS account to instantiate a ECR repository with a custom Docker image, a AWS HealthOmics (AHO) private workflow, and a lambda function that invokes the AHO workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for deploying the CloudFormation stack:\n",
    "1. Create a S3 bucket for storing required files in the same region as your cf stack\n",
    "2. Upload workflow definition files to S3\n",
    "3. Package and upload container code to S3\n",
    "4. Download and store ESM2 model weights in S3\n",
    "5. Deploy the CloudFormation stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 1-3. Create S3 bucket and upload workflow files, container code, and cf template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"  # Your desired region\n",
    "S3_BUCKET_NAME = \"protein_design_west2\"  # Your bucket name\n",
    "STACK_NAME = 'protein_design_stack'  # Your cf stack name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Function to create S3 bucket in specified region\n",
    "def create_s3_bucket(bucket_name, region):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in the specified region if it doesn't exist\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_name (str): Name of the S3 bucket to create\n",
    "    region (str): AWS region where the bucket should be created\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if bucket was created or already exists, False otherwise\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Check if bucket already exists\n",
    "        response = s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"404\" in str(e):\n",
    "            # Bucket doesn't exist, create it\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    # Special case for us-east-1 which doesn't accept LocationConstraint\n",
    "                    response = s3_client.create_bucket(\n",
    "                        Bucket=bucket_name\n",
    "                    )\n",
    "                else:\n",
    "                    response = s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={\n",
    "                            'LocationConstraint': region\n",
    "                        }\n",
    "                    )\n",
    "                print(f\"Successfully created bucket {bucket_name} in {region}\")\n",
    "                return True\n",
    "            except Exception as create_error:\n",
    "                print(f\"Error creating bucket: {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Error checking bucket: {e}\")\n",
    "            return False\n",
    "\n",
    "# Create zip file of container code\n",
    "def create_container_zip():\n",
    "    try:\n",
    "        shutil.make_archive('code', 'zip', 'container')\n",
    "        print(\"Successfully created code.zip from container directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating zip file: {e}\")\n",
    "\n",
    "# Upload workflow files, container code, and cf template to S3\n",
    "def upload_to_s3(bucket_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Upload workflow files\n",
    "    workflow_files = ['main.nf', 'nextflow.config', 'config.yaml', 'parameter-template.json']\n",
    "    for file in workflow_files:\n",
    "        try:\n",
    "            s3.upload_file(\n",
    "                f'aho_workflow/{file}', \n",
    "                bucket_name, \n",
    "                f'workflow/{file}'\n",
    "            )\n",
    "            print(f\"Uploaded {file} to s3://{bucket_name}/workflow/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file}: {e}\")\n",
    "    \n",
    "    # Upload container code zip\n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            'code.zip',\n",
    "            bucket_name,\n",
    "            'code.zip'\n",
    "        )\n",
    "        print(f\"Uploaded code.zip to s3://{bucket_name}/\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading code.zip: {e}\")\n",
    "\n",
    "    # Upload cf template\n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            'protein_design_stack.yaml',\n",
    "            bucket_name,\n",
    "            'templates/protein_design_stack.yaml'\n",
    "        )\n",
    "        print(f\"Uploaded cf template to s3://{bucket_name}/templates/\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading cf template: {e}\")\n",
    "\n",
    "\n",
    "# Define the CloudFormation parameters\n",
    "def write_cf_parameters(bucket_name):\n",
    "    '''Write the param JSON file for creating the cf stack'''\n",
    "    cf_parameters = [\n",
    "        {\n",
    "            \"ParameterKey\": \"S3BucketName\",\n",
    "            \"ParameterValue\": bucket_name\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"StackPrefix\",\n",
    "            \"ParameterValue\": \"protein-design\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"ApplicationName\",\n",
    "            \"ParameterValue\": \"HealthOmics-Workflow\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"WorkflowPath\",\n",
    "            \"ParameterValue\": \"workflow\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"SecretName\",\n",
    "            \"ParameterValue\": \"protein-design-secret\"  # Default value from the template\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Write parameters to cf_parameter.json file\n",
    "    with open('cf_parameters.json', 'w') as f:\n",
    "        json.dump(cf_parameters, f, indent=2)\n",
    "\n",
    "    print(f\"CloudFormation parameters written to cf_parameters.json\")\n",
    "    print(f\"File path: {os.path.abspath('cf_parameters.json')}\")\n",
    "\n",
    "# Create the S3 bucket if it doesn't exist\n",
    "bucket_created = create_s3_bucket(S3_BUCKET_NAME, REGION)\n",
    "\n",
    "if bucket_created:\n",
    "    # Create and write CloudFormation parameters\n",
    "    write_cf_parameters(S3_BUCKET_NAME)\n",
    "    \n",
    "    # Create zip and upload files\n",
    "    create_container_zip()\n",
    "    upload_to_s3(S3_BUCKET_NAME)\n",
    "else:\n",
    "    print(\"Failed to create or verify S3 bucket. CloudFormation parameters not written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Download model weights and store in s3\n",
    "We will run ML-guided directed evolution on a protein sequence by using pre-trained protein language model (PLM) to guide the mutations using the EvoProtGrad framework.\n",
    "To use a pretrained PLM, download this model [facebook/esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D/tree/main) from huggingface and store the weights at `s3://protein_design_west2/models/esm2_t6_8M_UR50D/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Deploy the CloudFormation stack using these AWS CLI commands:\n",
    "````\n",
    "# Deploy the CloudFormation stack\n",
    "aws cloudformation create-stack \\\n",
    "    --stack-name protein_design_stack \\\n",
    "    --template-url https://protein_design_west2.s3.amazonaws.com/templates/protein_design_stack.yaml \\\n",
    "    --parameters file://cf_parameters.json \\\n",
    "    --capabilities CAPABILITY_IAM CAPABILITY_AUTO_EXPAND CAPABILITY_NAMED_IAM \\\n",
    "    --region us-west-2\n",
    "\n",
    "# Monitor stack creation\n",
    "aws cloudformation describe-stacks \\\n",
    "    --stack-name protein_design_stack \\\n",
    "    --query 'Stacks[0].StackStatus'\n",
    "\n",
    "# Get stack outputs once complete\n",
    "aws cloudformation describe-stacks \\\n",
    "    --stack-name protein_design_stack \\\n",
    "    --query 'Stacks[0].Outputs'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we will create the Bedrock Agent with code and attach action groups to the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in environment variables to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve import path\n",
    "%store -r IMPORTS_PATH\n",
    "\n",
    "# Retrieve account info\n",
    "%store -r account_id\n",
    "%store -r region\n",
    "\n",
    "# Retrieve model lists\n",
    "%store -r agent_foundation_model\n",
    "\n",
    "%run $IMPORTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS clients and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Configure AWS clients\n",
    "session = boto3.Session()\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "bedrock = boto3.client('bedrock', REGION)\n",
    "cfn = boto3.client('cloudformation', REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CloudFormation Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize the CloudFormation client with the specific region\n",
    "cloudformation = boto3.client('cloudformation', region_name=REGION)\n",
    "\n",
    "def get_cloudformation_outputs(stack_name):\n",
    "    try:\n",
    "        response = cloudformation.describe_stacks(StackName=stack_name)\n",
    "        outputs = {}\n",
    "        for output in response['Stacks'][0]['Outputs']:\n",
    "            outputs[output['OutputKey']] = output['OutputValue']\n",
    "        return outputs\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting CloudFormation outputs: {e}\")\n",
    "        raise\n",
    "\n",
    "# Get the outputs from CloudFormation\n",
    "cf_outputs = get_cloudformation_outputs(STACK_NAME)\n",
    "print(\"CloudFormation Outputs:\")\n",
    "print(json.dumps(cf_outputs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_func_arn = cf_outputs[\"TriggerFunctionArn\"]\n",
    "trigger_func_name = \"WorkflowTriggerFunction\"\n",
    "monitor_func_arn = cf_outputs[\"MonitorFunctionArn\"]\n",
    "monitor_func_name = \"WorkflowMonitorFunction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent configuration\n",
    "agent_name = 'ProteinDesignAgent'\n",
    "agent_description = \"Agent for protein design using AWS HealthOmics workflows\"\n",
    "agent_instruction = \"\"\"You are an expert in protein design and optimization using AWS HealthOmics workflows. \n",
    "Your primary task is to help users run protein design optimization workflows and provide relevant insights.\n",
    "\n",
    "When providing your response:\n",
    "a. Start with a brief summary of your understanding of the user's query.\n",
    "b. Explain briefly the workflows you support and how each one does or does not meet the user's request.\n",
    "c. Explain the steps you're taking to address the query. Ask for clarifications from the user if required.\n",
    "d. Present the results of the workflow execution.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate agent with the desired configuration\n",
    "agents = AgentsForAmazonBedrock()\n",
    "\n",
    "protein_design_agent = agents.create_agent(\n",
    "    agent_name,\n",
    "    agent_description,\n",
    "    agent_instruction,\n",
    "    agent_foundation_model,\n",
    "    code_interpretation=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Extract useful agent information\n",
    "protein_design_agent_id = protein_design_agent[0]\n",
    "protein_design_agent_arn = f\"arn:aws:bedrock:{REGION}:{account_id}:agent/{protein_design_agent_id}\"\n",
    "\n",
    "print(f\"Agent created with ID: {protein_design_agent_id}\")\n",
    "print(f\"Agent ARN: {protein_design_agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Action Group for Workflow Trigger Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_defs = [\n",
    "    {\n",
    "        \"name\": \"trigger_aho_workflow\",\n",
    "        \"description\": \"Trigger the AWS HealthOmics workflow for protein design optimization\",\n",
    "        \"parameters\": {\n",
    "            \"seed_sequence\": {\n",
    "                \"description\": \"The input protein sequence to optimize\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"runName\": {\n",
    "                \"description\": \"Name for the workflow run (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"outputUri\": {\n",
    "                \"description\": \"S3 URI where the workflow outputs will be stored (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"esm_model_files\": {\n",
    "                \"description\": \"S3 directory storing ESM pLM model files (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"onehotcnn_model_files\": {\n",
    "                \"description\": \"S3 directory storing Onehot CNN predictor model files (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"output_type\": {\n",
    "                \"description\": \"Output type, can be 'best', 'last', or 'all' variants (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"parallel_chains\": {\n",
    "                \"description\": \"Number of MCMC chains to run in parallel (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"n_steps\": {\n",
    "                \"description\": \"Number of MCMC steps per chain (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"max_mutations\": {\n",
    "                \"description\": \"Maximum number of mutations per variant (optional)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"requireConfirmation\": \"DISABLED\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add action group with Lambda function\n",
    "agents.add_action_group_with_lambda(\n",
    "    agent_name=agent_name,\n",
    "    lambda_function_name=trigger_func_name,\n",
    "    source_code_file=trigger_func_arn,\n",
    "    agent_action_group_name=\"ProteinDesignTriggerWorkflow\",\n",
    "    agent_action_group_description=\"Actions for triggering AWS HealthOmics workflows for protein design\",\n",
    "    agent_functions=function_defs,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lambda Resource-Based Policy\n",
    "\n",
    "lambda_client = boto3.client('lambda', REGION)\n",
    "\n",
    "try:\n",
    "    # Add the new statement to the existing policy\n",
    "    response = lambda_client.add_permission(\n",
    "        FunctionName=trigger_func_arn,\n",
    "        StatementId=\"AllowBedrockAgentAccess\",\n",
    "        Action=\"lambda:InvokeFunction\",\n",
    "        Principal=\"bedrock.amazonaws.com\",\n",
    "        SourceArn=protein_design_agent_arn\n",
    "    )\n",
    "    \n",
    "    print(\"Resource policy added successfully.\")\n",
    "    print(\"Response:\", response)\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    print(\"Permission already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding permission: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Action Group for Workflow Monitor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_defs = [\n",
    "    {\n",
    "        \"name\": \"monitor_aho_workflow\",\n",
    "        \"description\": \"Monitor the status of a running AWS HealthOmics workflow and retrieve results when complete\",\n",
    "        \"parameters\": {\n",
    "            \"runId\": {\n",
    "                \"description\": \"The ID of the HealthOmics workflow run to monitor\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"waitForCompletion\": {\n",
    "                \"description\": \"Whether to wait for the workflow to complete before returning (optional, defaults to True)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"boolean\"\n",
    "            },\n",
    "            \"maxWaitTimeMinutes\": {\n",
    "                \"description\": \"Maximum time to wait for workflow completion in minutes (optional, defaults to 30)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"pollIntervalSeconds\": {\n",
    "                \"description\": \"Time between status checks in seconds (optional, defaults to 30)\",\n",
    "                \"required\": False,\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        },\n",
    "        \"requireConfirmation\": \"DISABLED\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add action group with Lambda function\n",
    "agents.add_action_group_with_lambda(\n",
    "    agent_name=agent_name,\n",
    "    lambda_function_name=monitor_func_name,\n",
    "    source_code_file=monitor_func_arn,\n",
    "    agent_action_group_name=\"ProteinDesignMonitorRuns\",\n",
    "    agent_action_group_description=\"Actions for monitoring AWS HealthOmics workflow runs for protein design\",\n",
    "    agent_functions=function_defs,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lambda Resource-Based Policy\n",
    "\n",
    "lambda_client = boto3.client('lambda', REGION)\n",
    "\n",
    "try:\n",
    "    # Add the new statement to the existing policy\n",
    "    response = lambda_client.add_permission(\n",
    "        FunctionName=monitor_func_arn,\n",
    "        StatementId=\"AllowBedrockAgentAccess\",\n",
    "        Action=\"lambda:InvokeFunction\",\n",
    "        Principal=\"bedrock.amazonaws.com\",\n",
    "        SourceArn=protein_design_agent_arn\n",
    "    )\n",
    "    \n",
    "    print(\"Resource policy added successfully.\")\n",
    "    print(\"Response:\", response)\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    print(\"Permission already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding permission: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent alias\n",
    "protein_design_agent_alias_id, protein_design_agent_alias_arn = agents.create_agent_alias(\n",
    "    protein_design_agent[0], 'v1'\n",
    ")\n",
    "\n",
    "# Store the alias ARN for future use\n",
    "%store protein_design_agent_alias_arn\n",
    "\n",
    "print(f\"Agent alias created with ID: {protein_design_agent_alias_id}\")\n",
    "print(f\"Agent alias ARN: {protein_design_agent_alias_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", REGION)\n",
    "session_id = str(uuid.uuid1())\n",
    "\n",
    "test_query = \"Run a protein optimization for sequence ACDEFGHIKLMNPQRSTVWY with 20 parallel chains and 200 steps\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.invoke_agent(\n",
    "    inputText=test_query,\n",
    "    agentId=protein_design_agent_id,\n",
    "    agentAliasId=protein_design_agent_alias_id,\n",
    "    sessionId=session_id,\n",
    "    enableTrace=True\n",
    ")\n",
    "\n",
    "print(\"Request sent to Agent:\\n{}\".format(response))\n",
    "print(\"====================\")\n",
    "print(\"Agent processing query now\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Initialize an empty string to store the answer\n",
    "answer = \"\"\n",
    "\n",
    "# Iterate through the event stream\n",
    "for event in response['completion']:\n",
    "    # Check if the event is a 'chunk' event\n",
    "    if 'chunk' in event:\n",
    "        chunk_obj = event['chunk']\n",
    "        if 'bytes' in chunk_obj:\n",
    "            # Decode the bytes and append to the answer\n",
    "            chunk_data = chunk_obj['bytes'].decode('utf-8')\n",
    "            answer += chunk_data\n",
    "\n",
    "print(\"Agent Answer: {}\".format(answer))\n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", REGION)\n",
    "session_id = str(uuid.uuid1())\n",
    "\n",
    "test_query = \"could you check the status of my protein design workflow 7988468\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.invoke_agent(\n",
    "    inputText=test_query,\n",
    "    agentId=protein_design_agent_id,\n",
    "    agentAliasId=protein_design_agent_alias_id,\n",
    "    sessionId=session_id,\n",
    "    enableTrace=True\n",
    ")\n",
    "\n",
    "print(\"Request sent to Agent:\\n{}\".format(response))\n",
    "print(\"====================\")\n",
    "print(\"Agent processing query now\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Initialize an empty string to store the answer\n",
    "answer = \"\"\n",
    "\n",
    "# Iterate through the event stream\n",
    "for event in response['completion']:\n",
    "    # Check if the event is a 'chunk' event\n",
    "    if 'chunk' in event:\n",
    "        chunk_obj = event['chunk']\n",
    "        if 'bytes' in chunk_obj:\n",
    "            # Decode the bytes and append to the answer\n",
    "            chunk_data = chunk_obj['bytes'].decode('utf-8')\n",
    "            answer += chunk_data\n",
    "\n",
    "print(\"Agent Answer: {}\".format(answer))\n",
    "print(\"====================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
